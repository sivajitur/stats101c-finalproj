{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------Libraries-----------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------Data Import-----------------------\n",
    "\n",
    "DF_Final = pd.read_csv('subset.csv')\n",
    "DF_Final = DF_Final.drop(columns=['Unnamed: 0', 'X'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------Use TF-IDF to choose dictionary-----------------------\n",
    "documents = []\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "X = DF_Final.Review\n",
    "stemmer = WordNetLemmatizer()\n",
    "for i in range(0, len(X)):\n",
    "    # Remove all the special characters, like parathesis\n",
    "    document = re.sub(r'\\W', ' ', str(X[i]))\n",
    "    # remove all single characters: like a, b, c, d\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Remove single characters from the startf\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    documents.append(document)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=0.1, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------Negative Sentiment Lexicon-----------------------------------------\n",
    "f_n = open(\"negative-words.txt\", \"r\")\n",
    "Negative_words = f_n.readlines()\n",
    "\n",
    "NEG = [i.rstrip('\\n') for i in Negative_words]\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=np.unique(NEG))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "neg = pd.DataFrame(X)\n",
    "neg.columns = NEG\n",
    "neg.to_csv('neg_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------Positive Sentiment Lexicon-----------------------------------------\n",
    "f_p = open(\"positive-words.txt\", \"r\")\n",
    "Positive_words = f_p.readlines()\n",
    "\n",
    "POS = [i.rstrip('\\n') for i in Positive_words]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=np.unique(POS))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "pos = pd.DataFrame(X)\n",
    "pos.columns = POS\n",
    "pos.to_csv('pos_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------Combining into one dataset-----------------------------------------\n",
    "num_pos = pos.sum(axis = 1)\n",
    "num_neg = neg.sum(axis = 1)\n",
    "joined = pd.DataFrame({'Positive': num_pos, 'Negative': num_neg})\n",
    "DF_Final = DF_Final.join(joined)\n",
    "DF_Final.to_csv('data_with_sentiment.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
